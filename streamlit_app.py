
import os
import io
import time
import json
import requests
import pandas as pd
import numpy as np
import streamlit as st
from sklearn.linear_model import LinearRegression
from datetime import datetime, timedelta

# Optional: load .env if present
try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

st.set_page_config(page_title="Agent Ph√¢n t√≠ch d·ªØ li·ªáu ƒëi·ªÉm s·ªë", layout="wide")

# -------------------------
# Helpers
# -------------------------
EXPECTED_COLUMNS = [
    "student_id", "student_name", "class", "grade_level",
    "subject", "exam_date", "score", "max_score", "weight"
]

def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [c.strip().lower() for c in df.columns]
    # map some common Vietnamese aliases if needed
    alias = {
        "ma_hs": "student_id",
        "ten_hs": "student_name",
        "lop": "class",
        "khoi": "grade_level",
        "mon": "subject",
        "ngay_kiem_tra": "exam_date",
        "diem": "score",
        "tong_diem": "max_score",
        "he_so": "weight",
    }
    df.rename(columns={k:v for k,v in alias.items() if k in df.columns}, inplace=True)
    missing = [c for c in EXPECTED_COLUMNS if c not in df.columns]
    if missing:
        st.warning(f"C·ªôt c√≤n thi·∫øu: {missing}. H√£y d√πng m·∫´u 'sample_data.csv' ƒë·ªÉ tham kh·∫£o c·∫•u tr√∫c.")
    return df

def load_data(uploaded_file) -> pd.DataFrame:
    if uploaded_file is None:
        # Load bundled sample
        sample_path = os.path.join(os.path.dirname(__file__), "sample_data.csv")
        df = pd.read_csv(sample_path)
    else:
        df = pd.read_csv(uploaded_file)
    df = _normalize_columns(df)
    # typing and preprocessing
    df["exam_date"] = pd.to_datetime(df["exam_date"])
    df["score"] = pd.to_numeric(df["score"], errors="coerce")
    df["max_score"] = pd.to_numeric(df["max_score"], errors="coerce")
    df["weight"] = pd.to_numeric(df["weight"], errors="coerce").fillna(1.0)
    df["score_pct"] = (df["score"] / df["max_score"]) * 100.0
    df["grade_level"] = df["grade_level"].astype(str)
    df["class"] = df["class"].astype(str)
    df["subject"] = df["subject"].astype(str)
    return df.dropna(subset=["score_pct"])

@st.cache_data
def compute_overview(df: pd.DataFrame):
    overall = {
        "S·ªë b√†i ki·ªÉm tra": len(df),
        "S·ªë h·ªçc sinh": df["student_id"].nunique(),
        "S·ªë l·ªõp": df["class"].nunique(),
        "S·ªë m√¥n": df["subject"].nunique(),
        "ƒêi·ªÉm TB (%)": round(df["score_pct"].mean(), 2),
        "T·ªâ l·ªá >= 50%": round((df["score_pct"] >= 50).mean() * 100, 2),
    }
    return overall

@st.cache_data
def agg_by_class_subject(df: pd.DataFrame):
    g = (df.groupby(["grade_level", "class", "subject"], as_index=False)
           .agg(so_bai=("score_pct", "count"),
                diem_tb=("score_pct", "mean"),
                ti_le_dat_50=("score_pct", lambda s: (s>=50).mean()*100)))
    g["diem_tb"] = g["diem_tb"].round(2)
    g["ti_le_dat_50"] = g["ti_le_dat_50"].round(2)
    return g

@st.cache_data
def latest_scores_by_student(df: pd.DataFrame):
    # L·∫•y b√†i ki·ªÉm tra m·ªõi nh·∫•t c·ªßa m·ªói h·ªçc sinh theo t·ª´ng m√¥n
    df_sorted = df.sort_values(["student_id", "subject", "exam_date"])
    latest = df_sorted.groupby(["student_id", "student_name", "class", "grade_level", "subject"], as_index=False).tail(1)
    return latest

@st.cache_data
@st.cache_data
def at_risk_students(df: pd.DataFrame, threshold_pct=50.0, drop_threshold=10.0):
    # S·∫Øp x·∫øp d·ªØ li·ªáu theo h·ªçc sinh, m√¥n, ng√†y ki·ªÉm tra
    df_sorted = df.sort_values(["student_id", "subject", "exam_date"])
    
    # T√≠nh ƒëi·ªÉm b√†i tr∆∞·ªõc v√† ƒë·ªô t·ª•t
    df_sorted["prev_score_pct"] = df_sorted.groupby(
        ["student_id", "subject"]
    )["score_pct"].shift(1)
    df_sorted["drop"] = df_sorted["prev_score_pct"] - df_sorted["score_pct"]
    
    # L·∫•y b√†i ki·ªÉm tra m·ªõi nh·∫•t c·ªßa m·ªói h·ªçc sinh cho t·ª´ng m√¥n
    latest = df_sorted.groupby(
        ["student_id", "student_name", "class", "grade_level", "subject"],
        as_index=False
    ).tail(1)
    
    # L·ªçc h·ªçc sinh c√≥ ƒëi·ªÉm th·∫•p ho·∫∑c t·ª•t m·∫°nh
    at_risk = latest[
        (latest["score_pct"] < threshold_pct) |
        (latest["drop"].fillna(0) >= drop_threshold)
    ].copy()
    
    # T√≠nh l·∫°i ƒëi·ªÅu ki·ªán tr·ª±c ti·∫øp tr√™n at_risk ƒë·ªÉ tr√°nh l·ªách index
    cond_low = at_risk["score_pct"] < threshold_pct
    cond_drop = at_risk["drop"].fillna(0) >= drop_threshold
    
    # Gh√©p l√Ω do
    at_risk["L√Ω do"] = np.where(cond_low, "ƒêi·ªÉm d∆∞·ªõi ng∆∞·ª°ng", "") + \
                       np.where(cond_drop, " | T·ª•t m·∫°nh", "")
    
    # S·∫Øp x·∫øp k·∫øt qu·∫£
    at_risk = at_risk.sort_values(["class", "subject", "score_pct"])
    
    return at_risk


@st.cache_data
def forecast_trend(df: pd.DataFrame, horizon_days=30):
    # D·ª± b√°o trung b√¨nh theo (kh·ªëi, l·ªõp, m√¥n) b·∫±ng h·ªìi quy tuy·∫øn t√≠nh th·ªùi gian
    out_rows = []
    for keys, sub in df.groupby(["grade_level", "class", "subject"]):
        daily = (sub.groupby("exam_date", as_index=False)["score_pct"].mean()
                   .sort_values("exam_date"))
        if len(daily) < 3:
            continue
        # fit linear regression on date -> score
        X = (daily["exam_date"].map(pd.Timestamp.toordinal)).values.reshape(-1,1)
        y = daily["score_pct"].values
        model = LinearRegression()
        model.fit(X, y)
        future_date = daily["exam_date"].max() + timedelta(days=horizon_days)
        y_pred = float(model.predict([[future_date.toordinal()]]))
        out_rows.append({
            "grade_level": keys[0],
            "class": keys[1],
            "subject": keys[2],
            "last_date": daily["exam_date"].max().date(),
            "forecast_date": future_date.date(),
            "current_avg": round(float(y[-1]), 2),
            "forecast_avg": round(y_pred, 2)
        })
    return pd.DataFrame(out_rows)

def summarize_for_llm(df: pd.DataFrame, top_k=5) -> str:
    # T·∫°o t√≥m t·∫Øt g·ªçn ƒë·ªÉ g·ª≠i cho LLM
    g = agg_by_class_subject(df).sort_values("diem_tb")
    worst = g.head(top_k)
    best = g.tail(top_k)
    overall = compute_overview(df)
    payload = {
        "overall": overall,
        "lowest_groups": worst.to_dict(orient="records"),
        "highest_groups": best.to_dict(orient="records"),
    }
    return json.dumps(payload, ensure_ascii=False, indent=2)

def ask_gemini(prompt: str, model_name: str, api_key: str) -> str:
    try:
        import google.generativeai as genai
    except Exception as e:
        return "Ch∆∞a c√†i g√≥i google-generativeai. Vui l√≤ng `pip install google-generativeai`."
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(model_name)
        resp = model.generate_content(prompt)
        return getattr(resp, "text", str(resp))
    except Exception as e:
        return f"L·ªói g·ªçi Gemini: {e}"

def ask_ollama(prompt: str, host: str, model_name: str) -> str:
    try:
        url = host.rstrip("/") + "/api/generate"
        r = requests.post(url, json={"model": model_name, "prompt": prompt, "stream": False}, timeout=120)
        r.raise_for_status()
        data = r.json()
        return data.get("response", json.dumps(data))
    except Exception as e:
        return f"L·ªói g·ªçi Ollama: {e}"

# -------------------------
# Sidebar controls
# -------------------------
st.sidebar.header("‚öôÔ∏è C·∫•u h√¨nh")
provider = st.sidebar.selectbox("Ch·ªçn nh√† cung c·∫•p LLM", ["Kh√¥ng d√πng LLM", "Gemini API", "Ollama (local)"])

gemini_model = st.sidebar.text_input("Gemini model", value=os.getenv("GEMINI_MODEL", "gemini-2.0-flash"))
gemini_key = st.sidebar.text_input("GEMINI_API_KEY", value=os.getenv("GEMINI_API_KEY", ""), type="password")

ollama_host = st.sidebar.text_input("Ollama HOST", value=os.getenv("OLLAMA_HOST", "http://localhost:11434"))
ollama_model = st.sidebar.text_input("Ollama model", value=os.getenv("OLLAMA_MODEL", "mistral"))

risk_threshold = st.sidebar.slider("Ng∆∞·ª°ng ƒëi·ªÉm r·ªßi ro (%)", min_value=0, max_value=100, value=50, step=1)
drop_threshold = st.sidebar.slider("Ng∆∞·ª°ng t·ª•t ƒëi·ªÉm m·∫°nh (ƒëi·ªÉm %)", min_value=0, max_value=50, value=10, step=1)
horizon_days = st.sidebar.slider("S·ªë ng√†y d·ª± b√°o", min_value=7, max_value=90, value=30, step=1)

st.sidebar.markdown("---")
uploaded = st.sidebar.file_uploader("T·∫£i CSV ƒëi·ªÉm (t√πy ch·ªçn)", type=["csv"])
st.sidebar.caption("N·∫øu kh√¥ng t·∫£i, ·ª©ng d·ª•ng d√πng `sample_data.csv`.")

# -------------------------
# Main
# -------------------------
st.title("üìä Agent Ph√¢n t√≠ch d·ªØ li·ªáu ƒëi·ªÉm s·ªë")
df = load_data(uploaded)

tab1, tab2, tab3, tab4, tab5 = st.tabs(["T·ªïng quan", "Theo l·ªõp & m√¥n", "H·ªçc sinh c·∫ßn h·ªó tr·ª£", "D·ª± b√°o", "AI nh·∫≠n x√©t"])

with tab1:
    st.subheader("T·ªïng quan nhanh")
    ov = compute_overview(df)
    c1, c2, c3, c4, c5, c6 = st.columns(6)
    for i, (k,v) in enumerate(ov.items()):
        with (c1, c2, c3, c4, c5, c6)[i]:
            st.metric(k, v)

    st.markdown("#### Xu h∆∞·ªõng ƒëi·ªÉm trung b√¨nh theo ng√†y")
    daily = df.groupby("exam_date", as_index=False)["score_pct"].mean()
    st.line_chart(daily.set_index("exam_date"))

with tab2:
    st.subheader("Ph√¢n t√≠ch theo kh·ªëi / l·ªõp / m√¥n")
    agg = agg_by_class_subject(df)
    st.dataframe(agg, use_container_width=True)
    st.markdown("#### Top nh√≥m ƒëi·ªÉm th·∫•p nh·∫•t")
    st.dataframe(agg.sort_values("diem_tb").head(10), use_container_width=True)
    st.markdown("#### Top nh√≥m ƒëi·ªÉm cao nh·∫•t")
    st.dataframe(agg.sort_values("diem_tb", ascending=False).head(10), use_container_width=True)

with tab3:
    st.subheader("H·ªçc sinh c·∫ßn h·ªó tr·ª£")
    risk = at_risk_students(df, threshold_pct=risk_threshold, drop_threshold=drop_threshold)
    st.dataframe(risk[["student_id","student_name","class","grade_level","subject","exam_date","score_pct","prev_score_pct","drop","L√Ω do"]], use_container_width=True)
    st.info(f"T·ªïng s·ªë h·ªçc sinh c·∫ßn h·ªó tr·ª£ (theo ti√™u ch√≠): {len(risk)}")

with tab4:
    st.subheader("D·ª± b√°o xu h∆∞·ªõng")
    fc = forecast_trend(df, horizon_days=horizon_days)
    if len(fc) == 0:
        st.warning("Ch∆∞a ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ d·ª± b√°o (c·∫ßn >= 3 m·ªëc th·ªùi gian cho m·ªói nh√≥m).")
    else:
        st.dataframe(fc, use_container_width=True)

with tab5:
    st.subheader("Nh·∫≠n x√©t t·ª´ AI")
    summary_json = summarize_for_llm(df, top_k=5)
    st.code(summary_json, language="json")
    user_question = st.text_area("B·∫°n mu·ªën AI nh·∫≠n x√©t g√¨? (v√≠ d·ª•: ƒë·ªÅ xu·∫•t bi·ªán ph√°p h·ªó tr·ª£ nh√≥m r·ªßi ro)", value="H√£y ƒë∆∞a ra 3 nh·∫≠n x√©t ch√≠nh v√† 3 khuy·∫øn ngh·ªã h√†nh ƒë·ªông.")
    if st.button("Ph√¢n t√≠ch b·∫±ng AI"):
        prompt = f"""
B·∫°n l√† chuy√™n gia h·ªçc ƒë∆∞·ªùng. D·ª±a v√†o JSON th·ªëng k√™ sau ƒë√¢y, h√£y:
- N√™u 3 nh·∫≠n x√©t ch√≠nh (xu h∆∞·ªõng, v·∫•n ƒë·ªÅ n·ªïi b·∫≠t).
- G·ª£i √Ω 3 h√†nh ƒë·ªông h·ªó tr·ª£ h·ªçc sinh v√† k·∫ø ho·∫°ch theo d√µi.
- N√≥i ng·∫Øn g·ªçn, c√≥ bullet, kh√¥ng gi·∫£i th√≠ch lan man.

D·ªÆ LI·ªÜU:
{summary_json}

Y√äU C·∫¶U TH√äM C·ª¶A NG∆Ø·ªúI D√ôNG:
{user_question}
"""
        with st.spinner("ƒêang g·ªçi LLM..."):
            if provider == "Gemini API":
                if not gemini_key:
                    st.error("Thi·∫øu GEMINI_API_KEY.")
                else:
                    answer = ask_gemini(prompt, gemini_model, gemini_key)
                    st.markdown(answer)
            elif provider == "Ollama (local)":
                answer = ask_ollama(prompt, ollama_host, ollama_model)
                st.markdown(answer)
            else:
                st.warning("B·∫°n ƒëang ·ªü ch·∫ø ƒë·ªô 'Kh√¥ng d√πng LLM'. H√£y ch·ªçn Gemini")
